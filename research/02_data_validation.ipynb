{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'C:/WINE_QUALITY_ML_PROJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\WINE_QUALITY_ML_PROJ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the entity (class skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    status_file: str\n",
    "    unzip_data_dir: Path\n",
    "    all_schema: dict # reading from schema.yaml, where we already store that what were the cols(features) present at the time of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create source configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only thing to update in configuration is def of te func get_data_validation_config\n",
    "# because ther eis we already made this constructor\n",
    "# here we define the same constructor again so we can run entire code here to check whether it is working fine or not\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_file_path = PARAMS_FILE_PATH,\n",
    "            schema_file_path = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath) # get the whole configuration\n",
    "        self.params = read_yaml(params_file_path) # get the schema\n",
    "        self.schema = read_yaml(schema_file_path) # get the parameters\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) # create artifacts folder, if exist already then don't throw error, handle this in utils.common.create_directories code\n",
    "\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation # get the configuration for data validation from whole configuration\n",
    "        schema = self.schema.COLUMNS # get the schema used in training\n",
    "\n",
    "        create_directories([config.root_dir]) # create the root_dir i.e. 'data validation' folder inside artifacts folder bcoz it' holding path -> artifacts/data_validation\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            status_file = config.status_file,\n",
    "            unzip_data_dir = config.unzip_data_dir,\n",
    "            all_schema = schema\n",
    "        )\n",
    "\n",
    "        return data_validation_config # return the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def validate_all_columns(self)-> bool:\n",
    "        try:\n",
    "            overall_validation_status = True\n",
    "\n",
    "            data = pd.read_csv(self.config.unzip_data_dir) # data we are fetching now\n",
    "            data_cols = data.dtypes.to_dict() # data type of cols along with cols\n",
    "\n",
    "            all_schema = self.config.all_schema # data used at the time of training\n",
    "\n",
    "            for column_name, data_type in data_cols.items():\n",
    "\n",
    "                if ((column_name in all_schema) and (all_schema[column_name] == data_type)): # if col is present and it's type is also matching\n",
    "                    val_status = True\n",
    "                    \n",
    "                    with open(self.config.status_file, 'a') as f:\n",
    "                        f.write(f\"Col Name: {column_name}, Col type: {data_type} \\nValidation Status: {val_status} \\n\")\n",
    "\n",
    "                elif ((column_name in all_schema) and (all_schema[column_name] != data_type)): # col is present but type is not matching\n",
    "                    val_status = False\n",
    "                    overall_validation_status = False\n",
    "                    \n",
    "                    with open(self.config.status_file, 'a') as f:\n",
    "                        f.write(f\"Col Name: {column_name}, Col type: {data_type}, Existing_Col_type: {all_schema[column_name]} \\nValidation Status: {val_status} \\n\")\n",
    "                    raise Exception('Data type is not matching, terminating the further data fetching process')\n",
    "\n",
    "                else: # col is not present\n",
    "                    val_status = False\n",
    "                    overall_validation_status = False\n",
    "                    \n",
    "                    with open(self.config.status_file, 'a') as f:\n",
    "                        f.write(f\"Col Name: {column_name}, Col type: {data_type} \\nValidation Status: {val_status} \\n\")\n",
    "                    raise Exception(\"Column does not exist, terminating the further data fetching process\")\n",
    "\n",
    "            return overall_validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-02 14:38:17,056: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-03-02 14:38:17,056: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-02 14:38:17,060: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-02 14:38:17,068: INFO: common: created directory at: artifacts]\n",
      "[2024-03-02 14:38:17,070: INFO: common: created directory at: artifacts/data_validation]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # creating an object so calling the configuration manager\n",
    "    # here don't be confused like in the ConfigurationManager Class Constructor we defined 3 arguments but here we are not passing any, So why it is not showing error?\n",
    "    # Because we assign all 3 arguments with \"default\" values i.e. path of config, params and schema yaml files\n",
    "    data_validation_config = config.get_data_validation_config() # fetching the object from config manager\n",
    "    data_validation = DataValidation(config = data_validation_config) # creating the component related to object, only make 1 component\n",
    "    data_validation.validate_all_columns() # fetching that component\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
