{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are writing in this notebook first, so we can understand easily, then we have to change this notebook into modular coding.\n",
    "\n",
    "i.e. entity -> source configuration -> component -> pipeline\n",
    "\n",
    "'src' is the main folder which contains all this, we created this 'research' folder to first experiment and check whether things are working or not.\n",
    "\n",
    "In modular coding, we have to just copy paste this notebook in respective folder and have to access them. I am writing this change in orange comment to highlight.\n",
    "\n",
    "Note : When we convert it into modular coding then it by default shows the packages which are not imported by marking them with yellow underline in the code, so we get idea that okay we need to import these things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to root dir to create artifacts folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'C:/WINE_QUALITY_ML_PROJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\WINE_QUALITY_ML_PROJ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data ingestion Entity (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path # in python, we have data type 'path' \n",
    "    source_url: str # but we don't have 'url' type datatype, hence have to use str for url\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data ingestion source configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are fetching file location from constants.py\n",
    "# and using two modules read_yaml to extract the paths and create_directories from utils.common.py\n",
    "\n",
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    # as soon as obj created, it should read the yaml files and create artifact folder related to data ingestion\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_file_path = PARAMS_FILE_PATH,\n",
    "            schema_file_path = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) # in the config.yaml, artifacts_root is key, fetch it's value (which is folder name)\n",
    "        # which we are going to create in the main dir and this create_directories and read_yaml are common funcs defined in utils \n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig: # in python class can also be return type\n",
    "        config = self.config.data_ingestion # in config 'data_ingestion' is nested dictionary\n",
    "                                            # we are just copying  its inner key value pairs to another variable namely 'config'\n",
    "        \n",
    "        create_directories([config.root_dir]) # root_dir is holding the path artifacts\\data_ingestion\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig( # creating the object of class\n",
    "            root_dir=config.root_dir,\n",
    "            source_url=config.source_url,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config # returning the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating data ingestion component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request # with the help of it, we download the data from the given url\n",
    "import zipfile # to unzip the file\n",
    "from pathlib import Path\n",
    "from mlProject.logging.logging import logger\n",
    "from mlProject.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config # fetching all the details from configuration manager\n",
    "\n",
    "    # downloading data file\n",
    "    def download_file(self):\n",
    "        # if data is not present on local device then get it from souce\n",
    "        # like when we download MNIST from torch.dataset, same thing happens\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url=self.config.source_url,\n",
    "                filename=self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\n",
    "\n",
    "        \n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-28 19:50:07,833: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-28 19:50:07,836: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-28 19:50:07,844: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-28 19:50:07,847: INFO: common: created directory at: artifacts]\n",
      "[2024-02-28 19:50:07,851: INFO: common: created directory at: artifacts/data_ingestion]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-28 19:50:09,388: INFO: 4022175765: artifacts/data_ingestion/data.zip download! with following info: \n",
      "Connection: close\n",
      "Content-Length: 26148\n",
      "Cache-Control: max-age=300\n",
      "Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox\n",
      "Content-Type: application/zip\n",
      "ETag: \"026ed31829e6874ab63255ebd95cc71ee7e6404d584eaf71ae3f079e260ffc7b\"\n",
      "Strict-Transport-Security: max-age=31536000\n",
      "X-Content-Type-Options: nosniff\n",
      "X-Frame-Options: deny\n",
      "X-XSS-Protection: 1; mode=block\n",
      "X-GitHub-Request-Id: 5C54:3393CE:C7453:D61AE:65DF4118\n",
      "Accept-Ranges: bytes\n",
      "Date: Wed, 28 Feb 2024 14:20:08 GMT\n",
      "Via: 1.1 varnish\n",
      "X-Served-By: cache-bom4750-BOM\n",
      "X-Cache: MISS\n",
      "X-Cache-Hits: 0\n",
      "X-Timer: S1709130008.366017,VS0,VE405\n",
      "Vary: Authorization,Accept-Encoding,Origin\n",
      "Access-Control-Allow-Origin: *\n",
      "Cross-Origin-Resource-Policy: cross-origin\n",
      "X-Fastly-Request-ID: 8382fb2b24e93752de12ce22b0907bb5626cf451\n",
      "Expires: Wed, 28 Feb 2024 14:25:08 GMT\n",
      "Source-Age: 0\n",
      "\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager() # step 1 -> fetching all the yaml files (config, params, schema) and creating folder 'artifacts'\n",
    "    \n",
    "    data_ingestion_config = config.get_data_ingestion_config() # step 2 -> now this object has all the info regarding data ingestion that\n",
    "    # from where it has to take data(url) and dump it(local machine) and unzipping and also create 'data_ingestion' folder inside 'artifacts' folder \n",
    "    \n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config) # step 3 -> creating an obj by passing all the arguments get in step 2.\n",
    "    \n",
    "    data_ingestion.download_file() # step 4\n",
    "    \n",
    "    data_ingestion.extract_zip_file() # step 5\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
